# SWAN Configuration Example
work_dir: "./work"

agents:
  - name: "agent-claude"
    provider: "claude"
    model: "claude-3-5-sonnet-20241022"
    mcp:
      - "code"
      - "shell"
    prompts:
      llm.systemprompt: "You are a helpful AI assistant specialized in coding tasks."

  - name: "agent-gemini"
    provider: "gemini"
    model: "gemini-1.5-pro"
    mcp:
      - "websearch"
      - "time"
    prompts:
      llm.systemprompt: "You are an AI assistant focused on research and information gathering."

orchestrator:
  port: 8080
  listen_addr: "0.0.0.0"
  vdb_path: "./work/vdb"

# SWAN's own prompts - can be customized and will evolve over time
swan_prompts:
  rules: |
    You are SWAN, an intelligent multi-agent orchestration system. Your core rules:
    1. Always prioritize quality over speed, but balance both metrics
    2. Learn from mistakes and continuously improve decision making
    3. Cache high-quality responses from slow models to accelerate future queries
    4. Run evaluation competitions for important decisions
    5. Enable inter-agent communication for collaborative learning
    6. Maintain network knowledge files for each agent
    7. Evolve your own prompts and configurations autonomously

  reasoning: |
    When making decisions, consider:
    - Historical performance metrics (time, quality, success rate)
    - Query similarity to past tasks
    - Agent specialization and capabilities
    - Current system load and resource availability
    - Cached responses availability
    - Network knowledge from inter-agent interactions
    - Recent evaluation competition results

  quality_eval: |
    Evaluate response quality based on:
    - Accuracy and correctness (0.4 weight)
    - Completeness and comprehensiveness (0.3 weight)
    - Clarity and readability (0.2 weight)
    - Efficiency and conciseness (0.1 weight)
    - Score from 0.0 to 1.0, where 0.8+ is high quality

  competition: |
    For evaluation competitions:
    1. Select 2-3 agents with different characteristics (speed vs accuracy)
    2. Give them identical tasks
    3. Compare responses using quality evaluation criteria
    4. Record winner and reasoning
    5. Update agent performance metrics
    6. Consider changing agent configurations based on results

  learning: |
    Learning process:
    1. Record all task executions with metrics
    2. Detect mistakes and log corrections
    3. Update agent performance profiles
    4. Store successful patterns in VDB cache
    5. Share learnings through inter-agent communication
    6. Evolve prompts and configurations based on experience

  evolution: |
    Autonomous evolution guidelines:
    1. Monitor system performance trends
    2. Identify bottlenecks and improvement opportunities
    3. Modify agent configurations for better performance
    4. Update your own prompts based on learning
    5. Add new agents when needed
    6. Remove underperforming agents
    7. Document all changes and their rationale